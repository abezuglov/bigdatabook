# Hadoop Distributed File System

## Introduction

It all started in October 2003, when  Sanjay Ghemawat et al published a paper on Google File System [^GFS_paper]. The authors shed light on how Google deals with storing large datasets on distributed systems reliably. The next year Sanjay Ghemawat and Jeffrey Dean published another paper this time on Map Reduce, the programming model for processing large datasets. The seminal papers laid the foundation for open source implementation of these technologies done by Apache, which is Apache Hadoop.

## Installation and running with Docker

As usual, we first need to install Hadoop framework. The official Hadoop resources [^Hadoop_links], contain comprehensive information on how to install the framework and use it. However, there are shortcuts and we will use one to save time on reading documentation, downloading, and configuring everything. The shortcut is another product named Docker that we install using the following command:

```console
# apt-get docker install
```

Essentially, Docker is a container engine that runs on top of Linux operating system and uses its namespaces and control groups. In Docker, an image is a lightweight list of instructions to create a container, and the container is a runnable instance of an image. There is a library of preconfigured Docker images available at http://hub.docker.com and we encourage you to try out some of them.

Here, we will use sequenceiq/hadoop[^hadoop_hub_link] that we will obtain by running:

```console
# docker pull sequenceiq/hadoop-docker
```

This command will pull the necessary libraries and prepares the image. Run 'docker image ls' to check if the image has been properly installed. Finally, to start the containerized Hadoop instance, all we need to do is:

```console
# docker run -it sequenceiq/hadoop-docker /etc/bootstrap.sh -bash
```

Here, apart from the self-explanatory 'run' command, the parameters -i and -t specify that the instance will be interactive and it will use the current terminal. After running this command, you should see the output similar to this:

![HDFS Container Running](images/figures/hdfs_container_run.png)

To stop the container and get back to the shell, use `exit' command.

## Interacting with HDFS
After everything is running and ready to go, it is time to get familiar with the environment. Hadoop is located in '/usr/local/hadoop' directory that has also been set as $HADOOP_PREFIX' environment variable, and it might be a good idea to change to that directory:

```console
# cd $HADOOP_PREFIX/bin
```

With all prep actions now complete, HDFS will accept the file system commands as follows:

```console
# ./hdfs dfs -<command>
```

Below is an example of running the `ls' command that works identical to `ls' everywhere else:

![HDFS ls output](images/figures/hdfs_ls_input.png)

Several other file system commands that can be used are: mkdir, cat, chown, chmod, cp, mkdir, mv, rm, rmdir. If you ever miss the command, and run just './hdfs dfs' hdfs will provide the complete list of available commands. 

| -<command> | action |
|---------|---------|
| mkdir   | make directory |
| 
|---------|---------|



[^GFS_paper]: The paper is available at https://static.googleusercontent.com/media/research.google.com/en//archive/gfs-sosp2003.pdf
[^Hadoop_links]: http://hadoop.apache.org/
[^hadoop_hub_link]: You can find the page with the image description here: https://hub.docker.com/r/sequenceiq/hadoop-docker/
[^hdfs_commands]: 